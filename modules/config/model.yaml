# =============================================================================
# Model Configuration
# =============================================================================
# LLM settings for transcription and summarization phases.
# Loaded by: api/llm_client.py, api/transcribe_api.py, api/summary_api.py
#
# Related config files:
#   - app.yaml: Application settings and feature toggles
#   - concurrency.yaml: API rate limits, retries, and parallelism
#   - image_processing.yaml: Image preprocessing settings
#
# Supported Providers:
#   openai     - GPT-5, GPT-4o, o-series (set OPENAI_API_KEY)
#   anthropic  - Claude 4.5, Claude 4, Claude 3 (set ANTHROPIC_API_KEY)
#   google     - Gemini 3, 2.5, 2.0, 1.5 (set GOOGLE_API_KEY)
#   openrouter - Multi-provider proxy (set OPENROUTER_API_KEY)
#
# Provider Detection:
#   Explicit: "provider:model" format (e.g., "openai:gpt-5-mini")
#   Automatic: Inferred from prefix (gpt- = openai, claude- = anthropic, etc.)

# =============================================================================
# Transcription Model
# =============================================================================
# Used for OCR/image-to-text conversion
transcription_model:
  name: "gpt-5-mini"      # Model identifier
  provider: "openai"   # Optional if model prefix is unambiguous
  max_output_tokens: 128000
  
  # Cross-provider reasoning (maps to native implementations):
  # OpenAI: reasoning_effort | Anthropic: extended_thinking | Google: thinking_level
  reasoning:
    effort: medium  # low | medium | high
  
  # OpenAI GPT-5 only: controls output verbosity
  text:
    verbosity: medium  # low | medium | high
  
  temperature: 1.0  # 0.0-2.0 (null = provider default)

# =============================================================================
# Summary Model
# =============================================================================
# Used for generating structured summaries from transcriptions
summary_model:
  name: "gpt-5-mini"
  provider: "openai"
  max_output_tokens: 128000
  
  reasoning:
    effort: medium
  
  text:
    verbosity: medium  # Lower verbosity for concise summaries
  
  temperature: 1.0

# =============================================================================
# Provider Examples
# =============================================================================
# Anthropic Claude 4.5 (with extended thinking):
#   transcription_model:
#     name: "claude-sonnet-4-5-20250929"
#     provider: "anthropic"
#     max_output_tokens: 16384
#     reasoning:
#       effort: medium  # Maps to budget_tokens: 4096
#     temperature: 1.0  # Required for extended thinking
#
# Google Gemini 2.5 (with thinking mode):
#   transcription_model:
#     name: "gemini-2.5-flash"
#     provider: "google"
#     max_output_tokens: 32768
#     reasoning:
#       effort: high  # Maps to thinking_level: "high"
#     temperature: 0.0
#
# Google Gemini 2.0 (no thinking mode):
#   transcription_model:
#     name: "gemini-2.0-flash"
#     provider: "google"
#     max_output_tokens: 8192
#     temperature: 0.0
#
# OpenRouter (access any model):
#   transcription_model:
#     name: "anthropic/claude-sonnet-4-5"
#     provider: "openrouter"
#     max_output_tokens: 8192
#     reasoning:
#       effort: medium
