# model_config.yaml
# Configuration for LLM models used in transcription and summarization.
# 
# Supported Providers:
#   - openai: OpenAI models (gpt-5, gpt-4o, o1, o3, etc.)
#   - anthropic: Anthropic Claude models (claude-3, claude-sonnet, etc.)
#   - google: Google Gemini models (gemini-2.0-flash, gemini-pro, etc.)
#   - openrouter: OpenRouter proxy (access multiple providers via unified API)
#
# Provider Detection:
#   - Explicit: Use "provider:model" format (e.g., "openai:gpt-5-mini")
#   - Automatic: Provider inferred from model name prefixes (gpt- = openai, claude- = anthropic, etc.)
#
# API Keys (set as environment variables):
#   - OPENAI_API_KEY: For OpenAI models
#   - ANTHROPIC_API_KEY: For Anthropic models
#   - GOOGLE_API_KEY: For Google Gemini models
#   - OPENROUTER_API_KEY: For OpenRouter proxy

transcription_model:
  # Model configuration for image transcription
  name: "gpt-5-mini"  # Model identifier (use "provider:model" for explicit provider)
  provider: "openai"  # Provider: openai, anthropic, google, openrouter (optional if using prefix)
  max_output_tokens: 128000  # Maximum output token budget
  
  # OpenAI-specific parameters (GPT-5 and o-series models)
  reasoning:
    effort: low  # Options: minimal, low, medium, high
  text:
    verbosity: medium  # Options: low, medium, high
  
  # Provider-agnostic parameters
  temperature: null  # null = use provider default; 0.0-2.0 for explicit control

summary_model:
  # Model configuration for text summarization
  name: "gpt-5-mini"  # Model identifier (use "provider:model" for explicit provider)
  provider: "openai"  # Provider: openai, anthropic, google, openrouter (optional if using prefix)
  max_output_tokens: 128000  # Maximum output token budget
  
  # OpenAI-specific parameters (GPT-5 and o-series models)
  reasoning:
    effort: low  # Options: minimal, low, medium, high
  text:
    verbosity: low  # Options: low, medium, high (low for more concise summaries)
  
  # Provider-agnostic parameters
  temperature: null  # null = use provider default; 0.0-2.0 for explicit control

# Example configurations for other providers:
#
# Anthropic Claude:
#   transcription_model:
#     name: "claude-sonnet-4-5-20250929"
#     provider: "anthropic"
#     max_output_tokens: 8192
#     temperature: 0.0
#
# Google Gemini:
#   transcription_model:
#     name: "gemini-2.0-flash"
#     provider: "google"
#     max_output_tokens: 8192
#     temperature: 0.0
#
# OpenRouter (access any model):
#   transcription_model:
#     name: "anthropic/claude-3-opus"
#     provider: "openrouter"
#     max_output_tokens: 4096
