# =============================================================================
# Concurrency and Retry Configuration
# =============================================================================
# Controls parallel processing, rate limiting, and retry behavior.
# Loaded by: modules/concurrency_helper.py, api/transcribe_api.py, api/summary_api.py
#
# Related config files:
#   - app.yaml: Application settings and feature toggles
#   - model.yaml: LLM provider and model settings
#   - image_processing.yaml: Image preprocessing settings

# =============================================================================
# IMAGE PROCESSING CONCURRENCY
# =============================================================================
# Local I/O operations: PDF page extraction and image preprocessing
# CPU/disk-bound (no external API calls)
image_processing:
  concurrency_limit: 24  # Parallel tasks (8-24 for SSD, lower for HDD)
  delay_between_tasks: 0  # No delay needed for local operations

# =============================================================================
# API CONCURRENCY SETTINGS
# =============================================================================
# Controls LLM API requests for transcription and summarization
# Applies to all providers: OpenAI, Anthropic, Google, OpenRouter
api_requests:
  # ---------------------------------------------------------------------------
  # Global Settings (apply to all API calls)
  # ---------------------------------------------------------------------------
  api_timeout: 900  # Request timeout (seconds). 900s for flex tier queuing
  
  # Rate limiting: [max_requests, window_seconds]
  # Multiple windows prevent bursting at different time scales
  rate_limits:
    - [120, 1]       # Per-second burst limit
    - [15000, 60]    # Per-minute sustained limit
    - [15000, 3600]  # Per-hour aggregate limit

  # ---------------------------------------------------------------------------
  # Transcription Phase
  # ---------------------------------------------------------------------------
  transcription:
    concurrency_limit: 1500  # Parallel API requests (OpenAI: 50-150, Anthropic: 5-10)
    delay_between_tasks: 0.1  # Stagger requests to smooth rate limit usage
    
    # OpenAI service tier: 'default' | 'flex' | 'priority'
    # flex = cheaper but may queue; priority = fastest but expensive
    service_tier: flex
  
  # ---------------------------------------------------------------------------
  # Summary Phase
  # ---------------------------------------------------------------------------
  summary:
    concurrency_limit: 1500  # Lower than transcription (summaries are larger)
    delay_between_tasks: 0.1
    service_tier: flex  # Cost optimization for batch summarization

# =============================================================================
# RETRY & BACKOFF CONFIGURATION
# =============================================================================
# Exponential backoff with jitter for API errors (OpenAI Cookbook best practices)
# Formula: wait_time = backoff_base * (multiplier ^ attempt) + random(jitter)
retry:
  # ---------------------------------------------------------------------------
  # API Error Retries
  # ---------------------------------------------------------------------------
  # Handles rate limits (429), timeouts, and server errors (5xx)
  max_attempts: 15  # Total attempts before giving up (OpenAI recommends 5-6)
  backoff_base: 0.5  # Initial wait time in seconds
  
  # Error-specific multipliers (higher = longer waits)
  backoff_multipliers:
    rate_limit: 2.0   # 429 Too Many Requests
    timeout: 1.5      # Connection/read timeouts
    server_error: 2.0 # 500-series errors
    other: 2.0        # Other retryable errors
  
  # Jitter prevents thundering herd (random delay added to backoff)
  jitter:
    min: 0.5
    max: 1.0
  
  # ---------------------------------------------------------------------------
  # Schema-Specific Retries
  # ---------------------------------------------------------------------------
  # Retry based on model output flags (not API errors)
  # Useful when model reports content issues that may resolve on retry
  schema_retries:
    # Transcription phase flags
    transcription:
      no_transcribable_text:  # Image contains no text
        enabled: false
        max_attempts: 0  # 0 = disabled (usually correct, no point retrying)
        backoff_base: 0.1
        backoff_multiplier: 1.5
      
      transcription_not_possible:  # Illegible/corrupted image
        enabled: true
        max_attempts: 3  # Retry may help with transient model issues or refusal
        backoff_base: 0.1
        backoff_multiplier: 1.5
    
    # Summary phase flags (aligned with summary_schema.json v2.1.0)
    # The schema now uses structured page_information instead of boolean flags:
    #   - page_number_type: "roman" | "arabic" | "none"
    #   - page_types: array of ["content", "blank", "table_of_contents", ...]
    #   - bullet_points: null for non-content pages
    summary:
      page_type_null_bullets:  # Non-content page with null bullet_points
        enabled: false
        max_attempts: 0  # Usually correct, no retry needed
        backoff_base: 0.5
        backoff_multiplier: 1.5
