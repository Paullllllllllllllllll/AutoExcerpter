# Concurrency and retry configuration for AutoExcerpter
# This file controls parallel processing behavior for both local operations
# and OpenAI API requests.

# =============================================================================
# IMAGE PROCESSING CONCURRENCY
# =============================================================================
# Controls local I/O operations: PDF extraction and image preprocessing
# These operations are CPU/disk-bound and don't hit external APIs
image_processing:
  # Maximum concurrent image extraction/processing tasks
  # Recommended: 8-24 for modern systems with SSD storage
  concurrency_limit: 24
  
  # Delay between starting tasks (seconds)
  # Set to 0 for local operations as there's no rate limiting concern
  delay_between_tasks: 0

# =============================================================================
# API CONCURRENCY & RETRY CONFIGURATION
# =============================================================================
# Controls OpenAI Responses API requests for transcription and summarization
# Based on OpenAI best practices for the Responses API
api_requests:
  # ---------------------------------------------------------------------------
  # Transcription API Settings
  # ---------------------------------------------------------------------------
  transcription:
    # Maximum concurrent API requests
    # Recommended: 50-150 for Tier 4+, lower for lower tiers
    # The Responses API handles concurrent requests well with proper rate limiting
    concurrency_limit: 150
    
    # Delay between starting API requests (seconds)
    # Small delay helps prevent bursts that could trigger rate limits
    delay_between_tasks: 0.05
    
    # OpenAI service tier for Responses API
    # Options: 'auto', 'default', 'flex', 'priority'
    # - 'flex': Lower cost, may queue during high demand (recommended for batch processing)
    # - 'default': Standard processing speed
    # - 'priority': Fastest processing, higher cost
    service_tier: flex
    
    # Number of requests per batch part file (for future Batch API support)
    batch_chunk_size: 50
  
  # ---------------------------------------------------------------------------
  # Summary API Settings
  # ---------------------------------------------------------------------------
  summary:
    # Maximum concurrent API requests for summarization
    # Generally lower than transcription as summaries are larger/more complex
    concurrency_limit: 500
    
    # Delay between starting API requests (seconds)
    delay_between_tasks: 0.05
    
    # OpenAI service tier for Responses API
    service_tier: flex
    
    # Batch configuration
    batch_chunk_size: 50

# =============================================================================
# RETRY & BACKOFF CONFIGURATION
# =============================================================================
# Controls retry behavior for failed API requests following OpenAI best practices
# Uses exponential backoff with jitter to prevent thundering herd problems
retry:
  # ---------------------------------------------------------------------------
  # General API Error Retries
  # ---------------------------------------------------------------------------
  # These retries apply to all API errors (rate limits, timeouts, server errors, etc.)
  
  # Maximum number of retry attempts for API errors
  # After this many failures, the request is abandoned
  # Recommended: 3-6 attempts (OpenAI recommends 5-6 for production)
  max_attempts: 5
  
  # Base backoff time (seconds) for exponential backoff
  # Actual wait time = base * (multiplier ^ attempt) + jitter
  # Recommended: 1.0 seconds
  backoff_base: 1.0
  
  # Backoff multipliers for different error types
  # Higher multipliers = longer waits between retries
  backoff_multipliers:
    rate_limit: 2.0      # 429 errors, "too many requests"
    timeout: 1.5          # Request timeout errors
    server_error: 2.0     # 500-series errors
    other: 2.0            # All other retryable errors
  
  # Jitter range to prevent thundering herd
  # Random value between [min, max] is added to backoff time
  jitter:
    min: 0.5
    max: 1.0
  
  # ---------------------------------------------------------------------------
  # Schema-Specific Content Retries (Transcription Phase)
  # ---------------------------------------------------------------------------
  # Additional retries when the model returns specific boolean flags in the
  # transcription schema, indicating content issues rather than API errors
  schema_retries:
    transcription:
      # Retry if model sets no_transcribable_text=true
      # This indicates the image contains no text to transcribe
      # Set to 0 to disable (recommended for most use cases)
      no_transcribable_text:
        enabled: true
        max_attempts: 1
        backoff_base: 0.5
        backoff_multiplier: 1.5
      
      # Retry if model sets transcription_not_possible=true
      # This indicates transcription is impossible (e.g., illegible, corrupted image)
      # Set to 0 to disable (recommended for most use cases)
      transcription_not_possible:
        enabled: true
        max_attempts: 3
        backoff_base: 0.5
        backoff_multiplier: 1.5
    
    # ---------------------------------------------------------------------------
    # Schema-Specific Content Retries (Summary Phase)
    # ---------------------------------------------------------------------------
    summary:
      # Retry if model sets contains_no_semantic_content=true
      # This indicates the page has no substantial content (e.g., blank page, TOC)
      # Set to 0 to disable (recommended for most use cases)
      contains_no_semantic_content:
        enabled: true
        max_attempts: 0
        backoff_base: 0.5
        backoff_multiplier: 1.5
      
      # Retry if model sets contains_no_page_number=true
      # This indicates the page has no visible page number
      # Set to 0 to disable (recommended for most use cases)
      contains_no_page_number:
        enabled: true
        max_attempts: 0
        backoff_base: 0.5
        backoff_multiplier: 1.5
